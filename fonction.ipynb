{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import missingno as msno \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(path_src,path_dst):\n",
    "    df = pd.read_csv(path_src, delim_whitespace=True, header=None)\n",
    "    df.columns = ['mpg','cylinders quantitative','displacement','horsepower','weight','acceleration','model year','origin','car name']\n",
    "    df.to_csv (path_dst, index=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_na(df):\n",
    "    null_columns=df.columns[df.isnull().any()]\n",
    "    df[null_columns].isnull().sum()\n",
    "\n",
    "    for column in null_columns:\n",
    "        print (\"La colonne \",column,\" a \", df[column].isnull().sum(), \" valeurs manquantes, soit \", df[column].isnull().sum()/(len(df[column])*0.01),\"% de valeurs manquantes\")\n",
    "    msno.matrix(df,color=(0, 0, 0.25),fontsize=15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_values_columns(columns_name, pourcentage,dataset):\n",
    "    dataset_modifie=dataset.copy()\n",
    "    columns=dataset_modifie.columns\n",
    "    if (sum(columns_name==columns) != 1):\n",
    "        print(\"Colonnes n'ont existante\")\n",
    "    \n",
    "    #nombre de valeur de la colonne\n",
    "    Nbr_values=len(dataset_modifie[columns_name])\n",
    "    \n",
    "    #nombre de valeur a supprimer\n",
    "    delete=int(Nbr_values*pourcentage/100)\n",
    "    \n",
    "    while(pd.isnull(dataset_modifie[columns_name]).sum()<delete):\n",
    "        #selection de l'index\n",
    "        index_delete=random.randrange(Nbr_values)      \n",
    "        dataset_modifie[columns_name][index_delete]= np.NaN\n",
    "        \n",
    "    return dataset_modifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_values_datasete(pourcentage,dataset):\n",
    "    dataset_modifie=dataset.copy()\n",
    "    columns=dataset_modifie.columns\n",
    "\n",
    "    #nombre de valeur dans le dataset\n",
    "    Nbr_values=dataset_modifie.shape[0]*dataset_modifie.shape[1]\n",
    "    \n",
    "    #nombre de valeur a supprimer\n",
    "    delete=int(Nbr_values*pourcentage/100)\n",
    "    \n",
    "    while(pd.isnull(dataset_modifie).sum().sum()<delete):\n",
    "        #selection de l'index\n",
    "        index_delete=random.randrange(Nbr_values)\n",
    "        columns_delete=random.choice(columns)\n",
    "        dataset_modifie[columns_delete][index_delete]= np.NaN\n",
    "        \n",
    "    return dataset_modifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_des_cas_concret(dataset):\n",
    "    dataset_modifie=dataset\n",
    "    index_with_nan = dataset_modifie.index[dataset_modifie.isnull().any(axis=1)]\n",
    "    dataset_modifie.drop(index_with_nan,0, inplace=True)\n",
    "    \n",
    "    return dataset_modifie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyse_des_données_disponibles(taux,dataset):\n",
    "    index=[]\n",
    "    \n",
    "    taux=int(taux)\n",
    "    \n",
    "    if taux >len(dataset.columns) or taux<0:\n",
    "        #pour que le taux soit compris entre \n",
    "        print(f\"Le taux doit etre inférieure a {dataset.columns} et supérieure a 0\")\n",
    "        exit(-1)\n",
    "   #tableau calculan les index des nan pour chaque colonne du dataframe et les concatenant dans une liste     \n",
    "    for i in dataset.columns:\n",
    "        indexcolumns=dataset.index[pd.isnull(df[i])]\n",
    "        if indexcolumns.shape !=0:\n",
    "            for cmpt in range(len(indexcolumns)):\n",
    "                index.append(indexcolumns[cmpt])\n",
    "                \n",
    "    #récuperation des indices et du nombre d'iteration de chaque indices\n",
    "    index_ligne=np.unique(index,return_counts=True)[0]\n",
    "    number_nan=np.unique(index,return_counts=True)[1]\n",
    "    \n",
    "    nan_sup_taux=np.where(number_nan>=taux)[0]\n",
    "    \n",
    "    #liste associant les array superieur aux taux de np.unique aux array réel\n",
    "    index_delete=[]\n",
    "    for i in nan_sup_taux:\n",
    "        index_delete.append(index_ligne[i])\n",
    "    \n",
    "    \n",
    "    #suppresion des ligne\n",
    "    dataset.drop(index_delete,0,inplace=True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyse_des_données_disponibles_V2(pourcentage,dataset):\n",
    "    index=[]\n",
    "    pourcentage=int(pourcentage)\n",
    "    dataset_modifie=dataset.copy()\n",
    "    columns=dataset_modifie.columns\n",
    "    #vérification pour chaque colonne\n",
    "    for i in columns:\n",
    "        #nombre de nan dans la colonne\n",
    "        number_of_nan=dataset_modifie[i].isnull().sum()\n",
    "        \n",
    "       #si nombre de nan/ taille de la taille superieur ou egale au taux on supp\n",
    "        if ((number_of_nan / dataset_modifie[i].shape[0])>=(pourcentage/100)) :\n",
    "            dataset_modifie=dataset_modifie.drop(i, axis=1)\n",
    "    return dataset_modifie\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_estimator_LR(name_columns_X,name_columns_Y,dataset,numbereofImputerchoosen):\n",
    "    \n",
    "    ######numbereofImputerchoosen####\n",
    "    \n",
    "    #1:SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #2:SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #3:IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
    "    #4:KNNImputer(n_neighbors=2)\n",
    "\n",
    "    \n",
    "    datet_modifie=dataset.copy()\n",
    "    \n",
    "    #définition du X et du Y\n",
    "    X= datet_modifie[name_columns_X]\n",
    "    y= datet_modifie[name_columns_Y]\n",
    "    \n",
    "    \n",
    "    #Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state =0)\n",
    "    \n",
    "    #Création des df train et test\n",
    "    dataframe_train=pd.concat([X_train, y_train],axis=1)\n",
    "    dataframe_test=pd.concat([X_test, y_test],axis=1)\n",
    "    \n",
    "    \n",
    "    #IMPUTATION DES DONNEE\n",
    "    \n",
    "    # definition du modele imputer\n",
    "    if numbereofImputerchoosen==1:\n",
    "        imputer=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    elif numbereofImputerchoosen==2:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    elif numbereofImputerchoosen==3:\n",
    "        imputer = IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
    "    elif numbereofImputerchoosen==4:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cmpt=0\n",
    "    for i in [dataframe_train,dataframe_test]:\n",
    "        buffer=i\n",
    "        #print(f\"avant imputation :{buffer.isnull().sum()}\")\n",
    "        # fit sur le dataset\n",
    "        imputer.fit(buffer)\n",
    "        # transform du dataset\n",
    "        buffer = imputer.transform(buffer)\n",
    "        #on retransforme en df\n",
    "        buffer = pd.DataFrame(buffer, columns = [name_columns_X,name_columns_Y])\n",
    "        #print(f\"Apres imputation :{buffer.isnull().sum()}\")\n",
    "        \n",
    "        if cmpt == 0:\n",
    "            dataframe_train=buffer.copy()\n",
    "        else:\n",
    "            dataframe_test=buffer.copy()\n",
    "        cmpt=cmpt+1\n",
    "        \n",
    "    #print(dataframe_train.isnull().sum())        \n",
    "#recuperation des X_train,y_train,X_test,y_test concatené\n",
    "    X_train=dataframe_train[name_columns_X]\n",
    "    y_train=dataframe_train[name_columns_Y]\n",
    "    X_test=dataframe_test[name_columns_X]\n",
    "    y_test=dataframe_test[name_columns_Y]\n",
    "    #remise de series --> DF\n",
    "    X_train=pd.DataFrame(data=X_train)\n",
    "    y_train=pd.DataFrame(data=y_train)\n",
    "    X_test=pd.DataFrame(data=X_test)\n",
    "    y_test=pd.DataFrame(data=y_test)\n",
    "    \n",
    "    \n",
    "#entrainement \n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    score_train=model.score(X_train, y_train)\n",
    "    #print(f\"le score du model est de :{model.score(X_train, y_train)}\")\n",
    "    #print(f\"Coef de la regresssion :{model.coef_[0][0]}\")\n",
    "    #print(f\"Intercept du modele: {model.intercept_[0]}\")\n",
    "#test\n",
    "    y_predict = model.predict(X_test)\n",
    "    model_mse = mean_squared_error(y_predict, y_test) \n",
    "    #print(model_mse)\n",
    "    #print(model.score(X_test, y_test))\n",
    "    score_test=model.score(X_test, y_test)\n",
    "    \n",
    "    return score_train,score_test,model_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_estimator_LR_v2(name_columns_X,name_columns_Y,dataset,numbereofImputerchoosen):\n",
    "    \n",
    "    ######numbereofImputerchoosen####\n",
    "    \n",
    "    #1:SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    #2:SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    #3:IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
    "    #4:KNNImputer(n_neighbors=2)\n",
    "\n",
    "    \n",
    "    datet_modifie=dataset.copy()\n",
    "    \n",
    "    #définition du X et du Y\n",
    "    #[\"ubi\",\"hbui\",\"vu\"]\n",
    "    X= datet_modifie[name_columns_X]\n",
    "    y= datet_modifie[name_columns_Y]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state =0)\n",
    "    \n",
    "    #Création des df train et test\n",
    "    dataframe_train=pd.concat([X_train, y_train],axis=1)\n",
    "    dataframe_test=pd.concat([X_test, y_test],axis=1)\n",
    "  \n",
    "    \n",
    "    #IMPUTATION DES DONNEE\n",
    "    \n",
    "    # definition du modele imputer\n",
    "    if numbereofImputerchoosen==1:\n",
    "        imputer=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    elif numbereofImputerchoosen==2:\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    elif numbereofImputerchoosen==3:\n",
    "        imputer = IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
    "    elif numbereofImputerchoosen==4:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    columns=dataframe_train.columns\n",
    "    cmpt=0\n",
    "    for i in [dataframe_train,dataframe_test]:\n",
    "        buffer=i\n",
    "        #print(f\"avant imputation :{buffer.isnull().sum()}\")\n",
    "        # fit sur le dataset\n",
    "        imputer.fit(buffer)\n",
    "        # transform du dataset\n",
    "        buffer = imputer.transform(buffer)\n",
    "        #on retransforme en df\n",
    "        buffer = pd.DataFrame(buffer, columns = [columns])\n",
    "        #print(f\"Apres imputation :{buffer.isnull().sum()}\")\n",
    "        \n",
    "        if cmpt == 0:\n",
    "            dataframe_train=buffer.copy()\n",
    "        else:\n",
    "            dataframe_test=buffer.copy()\n",
    "        cmpt=cmpt+1\n",
    "        \n",
    "    #print(dataframe_train.isnull().sum())        \n",
    "#recuperation des X_train,y_train,X_test,y_test concatené\n",
    "    X_train=dataframe_train[name_columns_X]\n",
    "    y_train=dataframe_train[name_columns_Y]\n",
    "    X_test=dataframe_test[name_columns_X]\n",
    "    y_test=dataframe_test[name_columns_Y]\n",
    "    #remise de series --> DF\n",
    "    X_train=pd.DataFrame(data=X_train)\n",
    "    y_train=pd.DataFrame(data=y_train)\n",
    "    X_test=pd.DataFrame(data=X_test)\n",
    "    y_test=pd.DataFrame(data=y_test)\n",
    "    \n",
    "    \n",
    "#entrainement \n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    score_train=model.score(X_train, y_train)\n",
    "    #print(f\"le score du model est de :{model.score(X_train, y_train)}\")\n",
    "    #print(f\"Coef de la regresssion :{model.coef_[0][0]}\")\n",
    "    #print(f\"Intercept du modele: {model.intercept_[0]}\")\n",
    "#test\n",
    "    y_predict = model.predict(X_test)\n",
    "    model_mse = mean_squared_error(y_predict, y_test) \n",
    "    #print(model_mse)\n",
    "    #print(model.score(X_test, y_test))\n",
    "    score_test=model.score(X_test, y_test)\n",
    "    \n",
    "    return score_train,score_test,model_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tab(list1,list2,list3,list4,colum_name_list1,colum_name_list2):\n",
    "    \n",
    "    #creation d'une colonne imputer name pour les df\n",
    "    imputer_name=[\"SimpleImputer_MEAN\",\"SimpleImputer_MEDIAN\",\"IterativeImputer\",\"KNNImputer\"]\n",
    "    df_imputer_name=pd.DataFrame(data=imputer_name)\n",
    "    df_imputer_name.columns=[\"imputer_name\"]\n",
    "\n",
    "    #list in tab\n",
    "    #TRAIN\n",
    "    #LIST1\n",
    "    dflist1=pd.DataFrame(data=list1)\n",
    "    dflist1.columns=[colum_name_list1]\n",
    "    #list in tab\n",
    "    #TEST\n",
    "    #LIST2\n",
    "    dflist2=pd.DataFrame(data=list2)\n",
    "    dflist2.columns=[colum_name_list1]\n",
    "    #list in tab\n",
    "    #TRAIN\n",
    "    #LIST3\n",
    "    dflist3=pd.DataFrame(data=list3)\n",
    "    dflist3.columns=[colum_name_list2]\n",
    "    #list in tab\n",
    "    #TEST\n",
    "    #LIST4\n",
    "    dflist4=pd.DataFrame(data=list4)\n",
    "    dflist4.columns=[colum_name_list2]\n",
    "    \n",
    "    #tabtrain list1 and liste 3\n",
    "    dftrain=pd.DataFrame(data=['train','train','train','train'])\n",
    "    dftrain.columns=[\"model\"]\n",
    "    df_result_train=pd.concat([df_imputer_name,dftrain,dflist1, dflist3],axis=1)\n",
    "    \n",
    "    \n",
    "    #tabtest list2 and list4\n",
    "    dftest=pd.DataFrame(data=['test','test','test','test'])\n",
    "    dftest.columns=[\"model\"]\n",
    "    df_result_test=pd.concat([df_imputer_name,dftest,dflist2, dflist4],axis=1)\n",
    "    \n",
    "    \n",
    "    #test and train\n",
    "    df_result=pd.concat([df_result_train,df_result_test],axis=0)\n",
    " \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
